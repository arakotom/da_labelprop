data:
  dep_sample: 1
  nb_class_in_bag: 10
  bag_size: 50
  n_class: 10
  model:
bagCSI:
  lr: 0.001
  n_epochs: 30
  param_bag:
    - 1
bagTopk:
  lr: 0.001
  n_epochs: 30
  topk: 1
  param_bag:
    - 1






















daLabelOT:  
  ent_weight: 0.
  clf_t_weight: 0
  div_weight: 0.
  n_epochs: 200
  epoch_start_g: 0
  start_align: 0
  use_div: False
  lr: 0.001
  lr_phi: 0.001
  #nblocks: 1
daLabelWD:  
  clf_t_weight: 0
  n_epochs: 30
  epoch_start_g: 0
  start_align: 1
  dist_loss_weight: 
    - 0
    - 0.1
    - 0.01
    - 0.001
    - 0.0001
  iter_domain_classifier: 1
  bag_loss_weight: 
    - 1
    - 25
    - 50
    - 100
  lr: 0.005


# tested
# small bag loss does not work (eg 0.1)
# larger bag loss works (eg 50) seems to work better
# with entropy loss, the model works better


